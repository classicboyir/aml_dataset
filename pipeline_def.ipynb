{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use of Azure ML Dataset as an approach to data mesh\n",
        "\n",
        "In this notebook, I'll demonstrate how to leverage Azure ML Datasets to approach a data mesh strategy for any model development activities across different compute targets, including databricks and AzureML by leveraging `Azure ML Pipelines`.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/pipeline.jpg\" /></div>\n",
        "\n",
        "*The AML Piepline Image*\n",
        "\n",
        "In order to run this example, you need to have a databricks cluster with ML Runtime created. The cluster needs azureml-sdk[databricks] package being installed.\n",
        "\n",
        "The overal idea is to create data lineage for the entire life cycle of a model, which starts with data processing and ends with model registration and deployment.\n",
        "A simple training excersice is picked to focus mostly on the use of AML Dataset.\n",
        "\n",
        "In this example, The data preprocessing happens on Databricks through `DatabricksStep` and the model training takes place on an AML Compute through `PythonScriptStep`. \n",
        "\n",
        "The first step receives three input AML Datasets and prepared for a model training excersice in the DatabricksStep. Later the final dataframe is saved as a `Parquet` or `Delta` tables. Finally, the written data is registered as a AML Dataset either `Tabular` for `Parquet` file format or `File` for `Delta` file format.\n",
        "\n",
        "Every time this step is executed, a new dataset is generated called `feature_titanic` as an AML Dataset. Which is then consumed by the next step. If the allow_reuse parameter on the `PythonScriptStep` constructor is set to True, then the output dataset registered from the previous run will be reused for the next step.\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/ADBStep.jpg\" /></div>\n",
        "\n",
        "*ADB Step details page; the input and output datasets.*\n",
        "\n",
        "The registered `AML Dataset` is passed to the subsequent `PythonScriptStep` which is meant for training. The data is read based on the incoming dataset type, either Delta or Parquet. For Parquet datasets, it's read through native `dataset.to_pandas_dataframe()` or read directly from the mount point to the ADLS storage with `pd.read_parquet()`. The Delta format should be read directly from the mounted ADLS sotrage through an open source package called `deltalake`. The data is converted into a Pandas dataframe, a transfomation is applyed to the dataframe and then a model is trained from the transformed dataframe. Finally, the model is registered and connected to the dataset used for training.\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/AMLStep.jpg\" /></div>\n",
        "\n",
        "*AML Step details page; the input and output datasets.*\n",
        "\n",
        "To register the model, an `AML Dataset` object is passed as a parameter to the `Model.register` function. This links the model to the dataset that was used to train the model.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/Model.jpg\" /></div>\n",
        "\n",
        "*Registered Model data tab; link to the feature_titanic AML Dataset.*\n",
        "\n",
        "This also helps us to connect the `AML Dataset` to the models as well.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/DatasetToModel.jpg\" /></div>\n",
        "\n",
        "*Model tab of the Featurized AML Dataset; link to the titanic_model AML Model.*\n",
        "\n",
        "During the lifecycle of the model and dataset, we leveraged `tags` parameter of the `register` function of `AML Datasets` and `AML Models`. This allows us to always keep and attach important parameters to the model and dataset objects. Parameters such as `dataset schema`, `input dataset`, `run_id`, etc.\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/DatasetTags.jpg\" /></div>\n",
        "\n",
        "*Taggs of the feature_titanic dataset. This identifies the input datasets, databricks feature store, data types of the final pandas dataframe, etc.*\n",
        "\n",
        "The `Environment` object for the `PythonScriptStep` is defined in a way to account for the required packages which include:\n",
        "* deltalake\n",
        "* sklearn\n",
        "* pandas\n",
        "* azureml-core\n",
        "\n",
        "*You need to make sure the `azureml-defaluts` is removed from the `Environment` object as it has conflict with a depancency in `deltalake` package.*\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/Environment.jpg\" /></div>\n",
        "\n",
        "*The environment definition for the AML Step*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import azureml.core\n",
        "import pandas as pd\n",
        "from azureml.core.runconfig import JarLibrary\n",
        "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
        "from azureml.exceptions import ComputeTargetException\n",
        "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\n",
        "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
        "from azureml.pipeline.steps import DatabricksStep, PythonScriptStep\n",
        "from azureml.core.datastore import Datastore\n",
        "from azureml.data.data_reference import DataReference\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "db_compute_name = \"ADBCluster\" # Databricks compute name\n",
        "\n",
        "databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
        "print('Compute target {} already exists'.format(db_compute_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.core.pipeline_output_dataset import PipelineOutputAbstractDataset\n",
        "\n",
        "def_blob_store = Datastore(ws, \"generalpurposeaccount\")\n",
        "print('Datastore {} will be used'.format(def_blob_store.name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651693315206
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "step_1_output = PipelineData(\"output\", datastore=def_blob_store)\n",
        "# ds_step_1_output = PipelineOutputAbstractDataset(step_1_output) # .as_dataset()\n",
        "ds_step_1_output = step_1_output.as_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "cluster_name = \"cpu-cluster-4\"\n",
        "compute_target = ComputeTarget(workspace=ws, name=cluster_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def register_dataset(datastore, dataset_name):\n",
        "    remote_path = f'dataset-demo/{dataset_name}/'\n",
        "    local_path = './data/titanic.csv'\n",
        "    datastore.upload_files(files = [local_path],\n",
        "                       target_path = remote_path,\n",
        "                       overwrite = True,\n",
        "                       show_progress = False)\n",
        "    \n",
        "    dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, remote_path)])\n",
        "    dataset = dataset.register(ws, name=dataset_name, create_new_version=True)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ds_titanic_1 = register_dataset(def_blob_store, 'titanic_1')\n",
        "ds_titanic_2 = register_dataset(def_blob_store, 'titanic_2')\n",
        "ds_titanic_3 = register_dataset(def_blob_store, 'titanic_3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "source_directory = \"./scripts\"\n",
        "\n",
        "isDeltaUsed = False\n",
        "\n",
        "if isDeltaUsed:\n",
        "    databricks_script_name = \"adb_run_delta.py\"\n",
        "    aml_script_name = 'aml_run_delta.py'\n",
        "else:\n",
        "    databricks_script_name = \"adb_run.py\"\n",
        "    aml_script_name = 'aml_run.py'\n",
        "\n",
        "feature_dataset_name = \"feature_titanic\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651694246481
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "dbNbStep = DatabricksStep(\n",
        "    name=\"ADBFeatureEng\",\n",
        "    outputs=[ds_step_1_output],\n",
        "    compute_target=databricks_compute,\n",
        "    existing_cluster_id=\"0517-170422-mxoe0n2x\", # this needs to be an Databricks Cluster with ML-Runtime - you need to install azureml-sdk[databrick] on the cluster\n",
        "    python_script_params=[\"--feature_set_1\", ds_titanic_1.name,\n",
        "                          \"--feature_set_2\", ds_titanic_2.name,\n",
        "                          \"--feature_set_3\", ds_titanic_3.name,\n",
        "                          '--output_datastore_name', def_blob_store.name,\n",
        "                          \"--output_feature_set_name\", feature_dataset_name],\n",
        "    permit_cluster_restart=True,\n",
        "    python_script_name=databricks_script_name,\n",
        "    source_directory=source_directory,\n",
        "    run_name='ADB_Feature_Eng',\n",
        "    allow_reuse=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.runconfig import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "tf_env_c = Environment('deltalake')\n",
        "\n",
        "conda_dep = CondaDependencies()\n",
        "\n",
        "conda_dep.add_pip_package(\"sklearn\")\n",
        "conda_dep.add_pip_package(\"deltalake\")\n",
        "conda_dep.remove_pip_package('azureml-defaults')\n",
        "conda_dep.add_pip_package('azureml-core')\n",
        "conda_dep.add_pip_package('pandas')\n",
        "\n",
        "# Adds dependencies to PythonSection of myenv\n",
        "tf_env_c.python.conda_dependencies=conda_dep\n",
        "\n",
        "tf_env_c = tf_env_c.register(workspace=ws)\n",
        "\n",
        "rcfg = RunConfiguration()\n",
        "rcfg.environment = tf_env_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "aml_step = PythonScriptStep(script_name=aml_script_name,\n",
        "                            name=\"AML Train\",\n",
        "                            source_directory=source_directory,\n",
        "                            inputs=[ds_step_1_output],\n",
        "                            compute_target=compute_target,\n",
        "                            arguments=['--data_folder', ds_step_1_output,\n",
        "                                       '--featureset_name', feature_dataset_name,\n",
        "                                       '--model_name', 'titanic_model'],\n",
        "                            allow_reuse=False,\n",
        "                            runconfig=rcfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651694252978
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "steps = [aml_step]\n",
        "pipeline = Pipeline(workspace=ws, steps=steps)\n",
        "pipeline_run = Experiment(ws, 'DB_FeatureStore').submit(pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pipeline_run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_run.wait_for_completion()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the pipeline is completed, then you can access the `dataset` information from the registered model by accessing the `datasets` properties of the registered model. In this example, you'll recieve a dictionary that the key is the name provided when the model was registered, `featurized data` in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "model = Model(ws, name='titanic_model')\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_datasets = model.datasets\n",
        "input_dataset = model_datasets['featurized data'][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "input_dataset.tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pdf = input_dataset.to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "Dataset.get_by_name"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
