{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leveraging Databricks Feature Store Along with AML Capabilities Using AML Pipelines\n",
        "\n",
        "In this notebook, I'll demonstrate how to leverage Azure ML Datasets to approach a data mesh strategy for any model development activities across different compute targets, including databricks and AzureML by leveraging `Azure ML Pipelines`. This notebook is the extension to the second [notebook](adbstep_and_automl.ipynb) by adding the Databricks Feature Store as the source for the AutoML step and Model Registration step as the final step.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/FeatureStorePipeline.jpg\" /></div>\n",
        "\n",
        "*The AML Piepline Image*\n",
        "\n",
        "In order to run this example, you need to have an AML Workspace with a Compute Cluster. In addition, you need an Azure Databricks cluster with **ML Runtime**. The cluster needs to have azureml-sdk[databricks] package installed.\n",
        "\n",
        "The overal idea is to create data lineage for the entire life cycle of a model, which starts with data processing and ends with model registration and deployment.\n",
        "A simple training excersice is picked to focus mostly on the flow of the experiement.\n",
        "\n",
        "In this example, there are two `DatabricksSteps`; the first is to prepare the base tables and feature tables with synthetic data, and the next step is to prepare the training data by leveraging the Databricks `FeatureLookup` capability. The prepared training and test datasets is then registered in `Databricks FeatureStore` as well as the `AML Dataset` and is passed to the `AutoMLStep` for training. Finally, the best model is registered as an AML Model in the AML Model Registry through `PythonScriptStep`. \n",
        "\n",
        "The first step generates and registers three base tables as `network`, `customer` and `ground_truth` tables. Then then base tables are registered as feature tables in `Databricks Feature Store` to be used in the next Databricks step which generates a training set out of these three tables. The final training and test sets are then registered in both Databricks and AML as feature tables and AML Datasets, respectively. Later the final dataframes (traing and test) are saved as a `Parquet` table. Finally, the saved data is registered as a AML Dataset as `TabularDataset` in `Parquet` file format.\n",
        "\n",
        "Every time the DatabricksSteps are executed, two new datasets are generated called `feature_network_fail_train` and `feature_network_fail_test` as AML TabularDatasets that are then passed to the AutoMLStep. If the allow_reuse parameter on the `DatabricksStep` constructor is set to True, then the output datasets registered from the previous run will be reused for the next step.\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/ADBStep_automl_ft.jpg\" /></div>\n",
        "\n",
        "*ADB Step details page; the input and output datasets.*\n",
        "\n",
        "Below is the output dataset which is registered as a Databricks Feature store:\n",
        "\n",
        "The registered `AML Dataset`s are passed to the subsequent `AutoMLStep` which is meant for training and testing of the AutoML Model. The data is read based on the incoming dataset type. Currently, AutoML supports csv and parquet for tabular datasets. Later the Delta will be supported as input datatype.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/AutoMLStepFT.jpg\" /></div>\n",
        "\n",
        "*AML Step details page; the input and output datasets.*\n",
        "\n",
        "Once the AutoMLStep is completed, the best model is passed to a subsequent step to register the best model. To register the model, the `AML Dataset` objects (one for training and one for testing) are passed as parameters to the `Model.register` function. This links the model to the datasets that were used the AutoML experiment.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/Model_AutoMLFT.jpg\" /></div>\n",
        "\n",
        "*Registered Model data tab; link to the `feature_network_fail_train` and `feature_network_fail_test` AML Datasets.*\n",
        "\n",
        "This also helps us to connect the `AML Dataset` to the models as well.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/DatasetToModelAutoMLFT.jpg\" /></div>\n",
        "\n",
        "*Model tab of the Featurized AML Dataset; link to the `network_failure_model` AML Model.*\n",
        "\n",
        "During the lifecycle of the model and dataset, we leveraged `tags` parameter of the `register` function of `AML Datasets` and `AML Models`. This allows us to always keep and attach important parameters to the model and dataset objects. Parameters such as `dataset schema`, `input dataset`, `run_id`, etc.\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/DatasetTagsFT.jpg\" /></div>\n",
        "\n",
        "*Taggs of the `feature_network_fail` train and test datasets. This identifies the input datasets, databricks feature store, data types of the final pandas dataframe, etc.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import azureml.core\n",
        "import pandas as pd\n",
        "from azureml.core.runconfig import JarLibrary\n",
        "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
        "from azureml.exceptions import ComputeTargetException\n",
        "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\n",
        "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
        "from azureml.pipeline.steps import DatabricksStep, PythonScriptStep\n",
        "from azureml.core.datastore import Datastore\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
        "from azureml.pipeline.steps import AutoMLStep\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "db_compute_name = \"ADBCluster\" # Databricks compute name\n",
        "\n",
        "databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
        "print('Compute target {} already exists'.format(db_compute_name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.core.pipeline_output_dataset import PipelineOutputAbstractDataset\n",
        "\n",
        "def_blob_store = Datastore(ws, \"generalpurposeaccount\")\n",
        "print('Datastore {} will be used'.format(def_blob_store.name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651693315206
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ds_step_1_train = PipelineData(\"output_train\", datastore=def_blob_store).as_dataset()\n",
        "ds_step_1_test = PipelineData(\"output_test\", datastore=def_blob_store).as_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "source_directory = \"./scripts\"\n",
        "\n",
        "prep_databricks_script_name = \"prepare_adb_feature_store.py\"\n",
        "databricks_script_name = \"adb_run_automl_featurestore.py\"\n",
        "\n",
        "dataset_name_train = \"feature_network_fail_train\"\n",
        "dataset_name_test = \"feature_network_fail_test\"\n",
        "ground_truth_name = \"ground_truth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_name = \"cpu-clu-pts\"\n",
        "compute_target_automl = ComputeTarget(workspace=ws, name=cluster_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reg_comp_name = \"<<compute_name>>\"\n",
        "reg_compute_target = ComputeTarget(workspace=ws, name=reg_comp_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_prep_step = DatabricksStep(\n",
        "    name=\"ADB_Create_Base_Feature_Tables\",\n",
        "    compute_target=databricks_compute,\n",
        "    existing_cluster_id=\"<<existing_cluster_id>>\",\n",
        "    python_script_params=[\"--ground-truth-tbl-name\", ground_truth_name],\n",
        "    permit_cluster_restart=True,\n",
        "    python_script_name=prep_databricks_script_name,\n",
        "    source_directory=source_directory,\n",
        "    run_name='ADB_Create_Base_Feature_Tables',\n",
        "    allow_reuse=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651694246481
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "db_feature_prap = DatabricksStep(\n",
        "    name=\"ADB_Feature_Prep\",\n",
        "    outputs=[ds_step_1_train, ds_step_1_test],\n",
        "    compute_target=databricks_compute,\n",
        "    existing_cluster_id=\"<<existing_cluster_id>>\",\n",
        "    python_script_params=[\"--ground-truth-tbl-name\", ground_truth_name,\n",
        "                          '--output_datastore_name', def_blob_store.name,\n",
        "                          \"--output_train_feature_set_name\", dataset_name_train, \n",
        "                          \"--output_test_feature_set_name\", dataset_name_test],\n",
        "    permit_cluster_restart=True,\n",
        "    python_script_name=databricks_script_name,\n",
        "    source_directory=source_directory,\n",
        "    run_name='ADB_Feature_Prep',\n",
        "    allow_reuse=True\n",
        ")\n",
        "db_feature_prap.run_after(db_prep_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change iterations to a reasonable number (50) to get better accuracy\n",
        "automl_settings = {\n",
        "    \"iteration_timeout_minutes\" : 10,\n",
        "    \"iterations\" : 2,\n",
        "    \"primary_metric\" : 'AUC_weighted',\n",
        "    \"n_cross_validations\": 5\n",
        "}\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'classification',\n",
        "                             debug_log = 'automated_ml_errors.log',\n",
        "                             compute_target = compute_target_automl,\n",
        "                             featurization = 'auto',\n",
        "                             training_data = ds_step_1_train.parse_parquet_files(),\n",
        "                             test_data = ds_step_1_test.parse_parquet_files(),\n",
        "                             label_column_name = 'label',\n",
        "                             **automl_settings)\n",
        "\n",
        "print(\"AutoML config created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = ws.get_default_datastore()\n",
        "metrics_output_name = 'metrics_output'\n",
        "best_model_output_name = 'best_model_output'\n",
        "\n",
        "metrics_data = PipelineData(name='metrics_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=metrics_output_name,\n",
        "                           training_output=TrainingOutput(type='Metrics'))\n",
        "\n",
        "model_data = PipelineData(name='model_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=best_model_output_name,\n",
        "                           training_output=TrainingOutput(type='Model'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_automlStep = AutoMLStep(name='AutoML_Classification',\n",
        "                                 automl_config=automl_config,\n",
        "                                 outputs=[model_data],\n",
        "                                 allow_reuse=True)\n",
        "\n",
        "print(\"trainWithAutomlStep created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-sdk\")\n",
        "\n",
        "rcfg = RunConfiguration(conda_dependencies=conda_dep)\n",
        "\n",
        "register_model_step = PythonScriptStep(script_name='register_model.py',\n",
        "                                       source_directory=source_directory,\n",
        "                                       name=\"Register_Best_Model\",\n",
        "                                       inputs=[model_data],\n",
        "                                       compute_target=reg_compute_target,\n",
        "                                       arguments=[\"--saved-model\", model_data, \n",
        "                                                  '--model-name' , 'network_failure_model', \n",
        "                                                  '--featureset-name-train', dataset_name_train, \n",
        "                                                  '--featureset-name-test', dataset_name_test],\n",
        "                                       allow_reuse=True,\n",
        "                                       runconfig=rcfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651694252978
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "steps = [register_model_step]\n",
        "pipeline = Pipeline(workspace=ws, steps=steps)\n",
        "pipeline_run = Experiment(ws, 'AutoML_ADB_FeatureStore').submit(pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pipeline_run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_run.wait_for_completion()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to access the model dataset properties in the production setting for deployment or model consumption\n",
        "\n",
        "Once the pipeline is completed, then you can access the `dataset` information from the registered model by accessing the `datasets` properties of the registered model. In this example, you'll recieve a dictionary that the key is the name provided when the model was registered, `featurized data` in this case.\n",
        "\n",
        "This is helpful if deployment setting of the retrieving dataset characteristics is important. In addition, you can use this method if you like to access the model and dataset information from outside of AML like Databricks or Kubernetes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "model = Model(ws, name='network_failure_model')\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_datasets = model.datasets\n",
        "featurized_data = model_datasets['featurized training data'][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "featurized_data.tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pdf = featurized_data.to_pandas_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Accessing the Dataset from outside of AML\n",
        "\n",
        "In some use-cases, you might want to access the AML Dataset from outside of AML such as Databricks. In order to do this, you can either access the registered data from the `Databricks Feature Store` as is provided in the first step in `DatabricksStep`, or simply calling the `Dataset.get_by_name` function to retrieve the dataset object and start exploring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_feature_dataset = Dataset.get_by_name(ws, feature_dataset_name)\n",
        "\n",
        "pdf_feature_dataset = ds_feature_dataset.to_pandas_dataframe()\n",
        "pdf_feature_dataset.head()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('azureml_py38')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
