{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Use of Azure ML Dataset as an approach to data mesh\n",
        "\n",
        "In this notebook, I'll demonstrate how to leverage Azure ML Datasets to approach a data mesh strategy for any model development activities across different compute targets, including databricks and AzureML by leveraging `Azure ML Pipelines`. This notebook is the extension to the first [notebook](pipeline_def.ipynb) by adding the AutoML and Model Registration step as subsequent steps to the DatabriksStep.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/pipeline_automl.jpg\" /></div>\n",
        "\n",
        "*The AML Piepline Image*\n",
        "\n",
        "In order to run this example, you need to have an AML Workspace with a Compute Cluster. In addition, you need an Azure Databricks cluster with ML Runtime. The cluster needs to have azureml-sdk[databricks] package installed.\n",
        "\n",
        "The overal idea is to create data lineage for the entire life cycle of a model, which starts with data processing and ends with model registration and deployment.\n",
        "A simple training excersice is picked to focus mostly on the use of AML Dataset.\n",
        "\n",
        "In this example, The data preprocessing happens on Databricks through `DatabricksStep` and the model training takes place on an AML Compute through `PythonScriptStep`. \n",
        "\n",
        "The first step receives three input AML Datasets and prepared for a model training excersice in the DatabricksStep. Later the final dataframe is saved as a `Parquet`. Finally, the saved data is registered as a AML Dataset as `TabularDataset` in `Parquet` file format. The spark dataframe is then registered in Azure Databricks `Feature Store` to be natively retrieved within Databricks.\n",
        "\n",
        "Every time the DatabricksStep is executed, two new datasets are generated called `feature_titanic_train` and `feature_titanic_test` as AML TabularDatasets that are then passed to the AutoMLStep. If the allow_reuse parameter on the `DatabricksStep` constructor is set to True, then the output datasets registered from the previous run will be reused for the next step.\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/ADBStep_automl.jpg\" /></div>\n",
        "\n",
        "*ADB Step details page; the input and output datasets.*\n",
        "\n",
        "Below is the output dataset which is registered as a Databricks Feature store:\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/DatabricksFeatureStoreAutoML.jpg\" /></div>\n",
        "\n",
        "*Feature Titanic dataset registered as an Azure Databricks Feature Store*\n",
        "\n",
        "The registered `AML Dataset`s are passed to the subsequent `AutoMLStep` which is meant for training and testing of the AutoML Model. The data is read based on the incoming dataset type. Currently, AutoML supports csv and parquet for tabular datasets. Later the Delta will be supported as input datatype.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/AutoMLStep.jpg\" /></div>\n",
        "\n",
        "*AML Step details page; the input and output datasets.*\n",
        "\n",
        "Once the AutoMLStep is completed, the best model is passed to a subsequent step to register the best model. To register the model, the `AML Dataset` objects (one for training and one for testing) are passed as parameters to the `Model.register` function. This links the model to the datasets that were used the AutoML experiment.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/Model_AutoML.jpg\" /></div>\n",
        "\n",
        "*Registered Model data tab; link to the feature_titanic AML Dataset.*\n",
        "\n",
        "This also helps us to connect the `AML Dataset` to the models as well.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/DatasetToModelAutoML.jpg\" /></div>\n",
        "\n",
        "*Model tab of the Featurized AML Dataset; link to the titanic_model AML Model.*\n",
        "\n",
        "During the lifecycle of the model and dataset, we leveraged `tags` parameter of the `register` function of `AML Datasets` and `AML Models`. This allows us to always keep and attach important parameters to the model and dataset objects. Parameters such as `dataset schema`, `input dataset`, `run_id`, etc.\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/DatasetTags.jpg\" /></div>\n",
        "\n",
        "*Taggs of the feature_titanic train and test datasets. This identifies the input datasets, databricks feature store, data types of the final pandas dataframe, etc.*\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azureml.core\n",
        "import pandas as pd\n",
        "from azureml.core.runconfig import JarLibrary\n",
        "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
        "from azureml.exceptions import ComputeTargetException\n",
        "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\n",
        "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
        "from azureml.pipeline.steps import DatabricksStep, PythonScriptStep\n",
        "from azureml.core.datastore import Datastore\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
        "from azureml.pipeline.steps import AutoMLStep\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_compute_name = \"ADBCluster\" # Databricks compute name\n",
        "\n",
        "databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
        "print('Compute target {} already exists'.format(db_compute_name))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.core.pipeline_output_dataset import PipelineOutputAbstractDataset\n",
        "\n",
        "def_blob_store = Datastore(ws, \"generalpurposeaccount\")\n",
        "print('Datastore {} will be used'.format(def_blob_store.name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def register_dataset(datastore, dataset_name):\n",
        "    remote_path = f'dataset-demo/{dataset_name}/'\n",
        "    local_path = './data/titanic.csv'\n",
        "    datastore.upload_files(files = [local_path],\n",
        "                       target_path = remote_path,\n",
        "                       overwrite = True,\n",
        "                       show_progress = False)\n",
        "    \n",
        "    dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, remote_path)])\n",
        "    dataset = dataset.register(ws, name=dataset_name, create_new_version=True)\n",
        "    return dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_titanic_1 = register_dataset(def_blob_store, 'titanic_1')\n",
        "ds_titanic_2 = register_dataset(def_blob_store, 'titanic_2')\n",
        "ds_titanic_3 = register_dataset(def_blob_store, 'titanic_3')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_step_1_train = PipelineData(\"output_train\", datastore=def_blob_store).as_dataset()\n",
        "ds_step_1_test = PipelineData(\"output_test\", datastore=def_blob_store).as_dataset()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651693315206
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory = \"./scripts\"\n",
        "\n",
        "databricks_script_name = \"adb_run_automl.py\"\n",
        "\n",
        "feature_dataset_name_train = \"feature_titanic_train\"\n",
        "feature_dataset_name_test = \"feature_titanic_test\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbNbStep = DatabricksStep(\n",
        "    name=\"ADB_Feature_Eng\",\n",
        "    outputs=[ds_step_1_train, ds_step_1_test],\n",
        "    compute_target=databricks_compute,\n",
        "    existing_cluster_id=\"<<cluster_id>>\",\n",
        "    python_script_params=[\"--feature_set_1\", ds_titanic_1.name,\n",
        "                          \"--feature_set_2\", ds_titanic_2.name,\n",
        "                          \"--feature_set_3\", ds_titanic_3.name,\n",
        "                          '--output_datastore_name', def_blob_store.name,\n",
        "                          \"--output_train_feature_set_name\", feature_dataset_name_train, \n",
        "                          \"--output_test_feature_set_name\", feature_dataset_name_test],\n",
        "    permit_cluster_restart=True,\n",
        "    python_script_name=databricks_script_name,\n",
        "    source_directory=source_directory,\n",
        "    run_name='ADB_Feature_Eng',\n",
        "    allow_reuse=True\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651694246481
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_name = \"<<compute_name>>\"\n",
        "compute_target = ComputeTarget(workspace=ws, name=cluster_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Change iterations to a reasonable number (50) to get better accuracy\n",
        "automl_settings = {\n",
        "    \"iteration_timeout_minutes\" : 10,\n",
        "    \"iterations\" : 2,\n",
        "    \"primary_metric\" : 'AUC_weighted',\n",
        "    \"n_cross_validations\": 5\n",
        "}\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'classification',\n",
        "                             debug_log = 'automated_ml_errors.log',\n",
        "                             compute_target = compute_target,\n",
        "                             featurization = 'auto',\n",
        "                             training_data = ds_step_1_train.parse_parquet_files(),\n",
        "                             test_data = ds_step_1_test.parse_parquet_files(),\n",
        "                             label_column_name = 'Survived',\n",
        "                             **automl_settings)\n",
        "                             \n",
        "print(\"AutoML config created.\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ws.get_default_datastore()\n",
        "metrics_output_name = 'metrics_output'\n",
        "best_model_output_name = 'best_model_output'\n",
        "\n",
        "metrics_data = PipelineData(name='metrics_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=metrics_output_name,\n",
        "                           training_output=TrainingOutput(type='Metrics'))\n",
        "model_data = PipelineData(name='model_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=best_model_output_name,\n",
        "                           training_output=TrainingOutput(type='Model'))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_automlStep = AutoMLStep(name='AutoML_Classification',\n",
        "                                 automl_config=automl_config,\n",
        "                                 outputs=[metrics_data, model_data],\n",
        "                                 allow_reuse=True)\n",
        "\n",
        "print(\"trainWithAutomlStep created.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "reg_comp_name = \"<<compute_name>>\"\n",
        "reg_compute_target = ComputeTarget(workspace=ws, name=reg_comp_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-sdk\")\n",
        "\n",
        "rcfg = RunConfiguration(conda_dependencies=conda_dep)\n",
        "\n",
        "register_model_step = PythonScriptStep(script_name='register_model.py',\n",
        "                                       source_directory=source_directory,\n",
        "                                       name=\"Register_Best_Model\",\n",
        "                                       inputs=[model_data],\n",
        "                                               # ds_step_1_train.parse_parquet_files().as_named_input('input_train'), \n",
        "                                               # ds_step_1_test.parse_parquet_files().as_named_input('input_test')],\n",
        "                                       compute_target=reg_compute_target,\n",
        "                                       arguments=[\"--saved-model\", model_data, \n",
        "                                                  '--model-name' , 'titanic_model', \n",
        "                                                  '--featureset-name-train', feature_dataset_name_train, \n",
        "                                                  '--featureset-name-test', feature_dataset_name_test],\n",
        "                                       allow_reuse=True,\n",
        "                                       runconfig=rcfg)\n",
        "\n",
        "# register_model_step.run_after(train_automlStep)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [dbNbStep, train_automlStep, register_model_step]\n",
        "pipeline = Pipeline(workspace=ws, steps=steps)\n",
        "pipeline_run = Experiment(ws, 'DB_FeatureStore_AutoML_Register').submit(pipeline)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651694252978
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run.wait_for_completion()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to access the model dataset properties in the production setting for deployment or model consumption\n",
        "\n",
        "Once the pipeline is completed, then you can access the `dataset` information from the registered model by accessing the `datasets` properties of the registered model. In this example, you'll recieve a dictionary that the key is the name provided when the model was registered, `featurized data` in this case.\n",
        "\n",
        "This is helpful if deployment setting of the retrieving dataset characteristics is important. In addition, you can use this method if you like to access the model and dataset information from outside of AML like Databricks or Kubernetes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "model = Model(ws, name='titanic_model')\n",
        "model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_datasets = model.datasets\n",
        "featurized_data = model_datasets['featurized training data'][0]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featurized_data.tags"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = featurized_data.to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing the Dataset from outside of AML\n",
        "\n",
        "In some use-cases, you might want to access the AML Dataset from outside of AML such as Databricks. In order to do this, you can either access the registered data from the `Databricks Feature Store` as is provided in the first step in `DatabricksStep`, or simply calling the `Dataset.get_by_name` function to retrieve the dataset object and start exploring."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ds_feature_dataset = Dataset.get_by_name(ws, feature_dataset_name)\n",
        "\n",
        "pdf_feature_dataset = ds_feature_dataset.to_pandas_dataframe()\n",
        "pdf_feature_dataset.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}