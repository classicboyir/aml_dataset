{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Use of Azure ML Dataset as an approach to data mesh\n",
        "\n",
        "In this notebook, I'll demonstrate how to leverage Azure ML Datasets to approach a data mesh strategy for any model development activities across different compute targets, including databricks and AzureML by leveraging `Azure ML Pipelines`. This notebook is the extension to the first [notebook](pipeline_def.ipynb) by adding the AutoML and Model Registration step as subsequent steps to the DatabriksStep.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/pipeline_automl.jpg\" /></div>\n",
        "\n",
        "*The AML Piepline Image*\n",
        "\n",
        "In order to run this example, you need to have an AML Workspace with a Compute Cluster. In addition, you need an Azure Databricks cluster with ML Runtime. The cluster needs to have azureml-sdk[databricks] package installed.\n",
        "\n",
        "The overal idea is to create data lineage for the entire life cycle of a model, which starts with data processing and ends with model registration and deployment.\n",
        "A simple training excersice is picked to focus mostly on the use of AML Dataset.\n",
        "\n",
        "In this example, The data preprocessing happens on Databricks through `DatabricksStep` and the model training takes place on an AML Compute through `PythonScriptStep`. \n",
        "\n",
        "The first step receives three input AML Datasets and prepared for a model training excersice in the DatabricksStep. Later the final dataframe is saved as a `Parquet`. Finally, the saved data is registered as a AML Dataset as `TabularDataset` in `Parquet` file format. The spark dataframe is then registered in Azure Databricks `Feature Store` to be natively retrieved within Databricks.\n",
        "\n",
        "Every time the DatabricksStep is executed, two new datasets are generated called `feature_titanic_train` and `feature_titanic_test` as AML TabularDatasets that are then passed to the AutoMLStep. If the allow_reuse parameter on the `DatabricksStep` constructor is set to True, then the output datasets registered from the previous run will be reused for the next step.\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/ADBStep_automl.jpg\" /></div>\n",
        "\n",
        "*ADB Step details page; the input and output datasets.*\n",
        "\n",
        "Below is the output dataset which is registered as a Databricks Feature store:\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/DatabricksFeatureStoreAutoML.jpg\" /></div>\n",
        "\n",
        "*Feature Titanic dataset registered as an Azure Databricks Feature Store*\n",
        "\n",
        "The registered `AML Dataset`s are passed to the subsequent `AutoMLStep` which is meant for training and testing of the AutoML Model. The data is read based on the incoming dataset type. Currently, AutoML supports csv and parquet for tabular datasets. Later the Delta will be supported as input datatype.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/AutoMLStep.jpg\" /></div>\n",
        "\n",
        "*AML Step details page; the input and output datasets.*\n",
        "\n",
        "Once the AutoMLStep is completed, the best model is passed to a subsequent step to register the best model. To register the model, the `AML Dataset` objects (one for training and one for testing) are passed as parameters to the `Model.register` function. This links the model to the datasets that were used the AutoML experiment.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/Model_AutoML.jpg\" /></div>\n",
        "\n",
        "*Registered Model data tab; link to the feature_titanic AML Dataset.*\n",
        "\n",
        "This also helps us to connect the `AML Dataset` to the models as well.\n",
        "\n",
        "<div style=\"text-align:center; width: 1000px\"><img src=\"./assets/DatasetToModelAutoML.jpg\" /></div>\n",
        "\n",
        "*Model tab of the Featurized AML Dataset; link to the titanic_model AML Model.*\n",
        "\n",
        "During the lifecycle of the model and dataset, we leveraged `tags` parameter of the `register` function of `AML Datasets` and `AML Models`. This allows us to always keep and attach important parameters to the model and dataset objects. Parameters such as `dataset schema`, `input dataset`, `run_id`, etc.\n",
        "\n",
        "<div style=\"text-align:center; width: 500px\"><img src=\"./assets/DatasetTags.jpg\" /></div>\n",
        "\n",
        "*Taggs of the feature_titanic train and test datasets. This identifies the input datasets, databricks feature store, data types of the final pandas dataframe, etc.*\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azureml.core\n",
        "import pandas as pd\n",
        "from azureml.core.runconfig import JarLibrary\n",
        "from azureml.core.compute import ComputeTarget, DatabricksCompute\n",
        "from azureml.exceptions import ComputeTargetException\n",
        "from azureml.core import Workspace, Environment, Experiment, Datastore, Dataset, ScriptRunConfig\n",
        "from azureml.pipeline.core import Pipeline, PipelineData, TrainingOutput\n",
        "from azureml.pipeline.steps import DatabricksStep, PythonScriptStep\n",
        "from azureml.core.datastore import Datastore\n",
        "from azureml.data.data_reference import DataReference\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "from azureml.pipeline.core import PipelineData, TrainingOutput\n",
        "from azureml.pipeline.steps import AutoMLStep\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "# Check core SDK version number\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SDK version: 1.40.0\n"
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Performing interactive authentication. Please follow the instructions on the terminal.\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-912041e0151d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubscription_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/core/workspace.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(path, auth, _logger, _file_name)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found the config file in: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         return Workspace.get(\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0mworkspace_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/core/workspace.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(name, auth, subscription_id, resource_group, location, cloud, id)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \"\"\"\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInteractiveLoginAuthentication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         return Workspace(\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/core/authentication.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, force, tenant_id, cloud)\u001b[0m\n\u001b[1;32m    575\u001b[0m                     print(\"Performing interactive authentication. Please follow the instructions \"\n\u001b[1;32m    576\u001b[0m                           \"on the terminal.\")\n\u001b[0;32m--> 577\u001b[0;31m                     \u001b[0mperform_interactive_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtenant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtenant_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcloud_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloud_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallback_az_cli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interactive authentication successfully completed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/_base_sdk_common/common.py\u001b[0m in \u001b[0;36mperform_interactive_login\u001b[0;34m(username, password, service_principal, tenant, allow_no_subscriptions, identity, use_device_code, use_cert_sn_issuer, cloud_type)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0mcredential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServicePrincipalAuth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_credential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecret_or_certificate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         subscriptions = profile.login(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/_vendor/azure_cli_core/_profile.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(self, interactive, username, password, is_service_principal, tenant, scopes, use_device_code, allow_no_subscriptions, use_cert_sn_issuer, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_device_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0muser_identity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin_with_device_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0muser_identity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin_with_auth_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/_vendor/azure_cli_core/auth/identity.py\u001b[0m in \u001b[0;36mlogin_with_device_code\u001b[0;34m(self, scopes, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \"Fail to create device flow. Err: %s\" % json.dumps(flow, indent=4))\n\u001b[1;32m    131\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsal_app\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire_token_by_device_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# By default it will block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msal/application.py\u001b[0m in \u001b[0;36macquire_token_by_device_flow\u001b[0;34m(self, flow, claims_challenge, **kwargs)\u001b[0m\n\u001b[1;32m   1625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACQUIRE_TOKEN_BY_DEVICE_FLOW_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m             correlation_id=flow.get(self.DEVICE_FLOW_CORRELATION_ID))\n\u001b[0;32m-> 1627\u001b[0;31m         response = _clean_up(self.client.obtain_token_by_device_flow(\n\u001b[0m\u001b[1;32m   1628\u001b[0m             \u001b[0mflow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             data=dict(\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/msal/oauth2cli/oauth2.py\u001b[0m in \u001b[0;36mobtain_token_by_device_flow\u001b[0;34m(self, flow, exit_condition, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mexit_condition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shorten each round, to make exit more responsive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     def _build_auth_request_uri(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_compute_name = \"ADBCluster\" # Databricks compute name\n",
        "\n",
        "databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
        "print('Compute target {} already exists'.format(db_compute_name))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core import PipelineParameter\n",
        "from azureml.pipeline.core.pipeline_output_dataset import PipelineOutputAbstractDataset\n",
        "\n",
        "def_blob_store = Datastore(ws, \"generalpurposeaccount\")\n",
        "print('Datastore {} will be used'.format(def_blob_store.name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def register_dataset(datastore, dataset_name):\n",
        "    remote_path = f'dataset-demo/{dataset_name}/'\n",
        "    local_path = './data/titanic.csv'\n",
        "    datastore.upload_files(files = [local_path],\n",
        "                       target_path = remote_path,\n",
        "                       overwrite = True,\n",
        "                       show_progress = False)\n",
        "    \n",
        "    dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, remote_path)])\n",
        "    dataset = dataset.register(ws, name=dataset_name, create_new_version=True)\n",
        "    return dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_titanic_1 = register_dataset(def_blob_store, 'titanic_1')\n",
        "ds_titanic_2 = register_dataset(def_blob_store, 'titanic_2')\n",
        "ds_titanic_3 = register_dataset(def_blob_store, 'titanic_3')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_step_1_train = PipelineData(\"output_train\", datastore=def_blob_store).as_dataset()\n",
        "ds_step_1_test = PipelineData(\"output_test\", datastore=def_blob_store).as_dataset()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651693315206
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory = \"./scripts\"\n",
        "\n",
        "databricks_script_name = \"adb_run_automl.py\"\n",
        "\n",
        "feature_dataset_name_train = \"feature_titanic_train\"\n",
        "feature_dataset_name_test = \"feature_titanic_test\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbNbStep = DatabricksStep(\n",
        "    name=\"ADB_Feature_Eng\",\n",
        "    outputs=[ds_step_1_train, ds_step_1_test],\n",
        "    compute_target=databricks_compute,\n",
        "    existing_cluster_id=\"<<cluster_id>>\",\n",
        "    python_script_params=[\"--feature_set_1\", ds_titanic_1.name,\n",
        "                          \"--feature_set_2\", ds_titanic_2.name,\n",
        "                          \"--feature_set_3\", ds_titanic_3.name,\n",
        "                          '--output_datastore_name', def_blob_store.name,\n",
        "                          \"--output_train_feature_set_name\", feature_dataset_name_train, \n",
        "                          \"--output_test_feature_set_name\", feature_dataset_name_test],\n",
        "    permit_cluster_restart=True,\n",
        "    python_script_name=databricks_script_name,\n",
        "    source_directory=source_directory,\n",
        "    run_name='ADB_Feature_Eng',\n",
        "    allow_reuse=True\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651694246481
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_name = \"<<compute_name>>\"\n",
        "compute_target = ComputeTarget(workspace=ws, name=cluster_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Change iterations to a reasonable number (50) to get better accuracy\n",
        "automl_settings = {\n",
        "    \"iteration_timeout_minutes\" : 10,\n",
        "    \"iterations\" : 2,\n",
        "    \"primary_metric\" : 'AUC_weighted',\n",
        "    \"n_cross_validations\": 5\n",
        "}\n",
        "\n",
        "automl_config = AutoMLConfig(task = 'classification',\n",
        "                             debug_log = 'automated_ml_errors.log',\n",
        "                             compute_target = compute_target,\n",
        "                             featurization = 'auto',\n",
        "                             training_data = ds_step_1_train.parse_parquet_files(),\n",
        "                             test_data = ds_step_1_test.parse_parquet_files(),\n",
        "                             label_column_name = 'Survived',\n",
        "                             **automl_settings)\n",
        "                             \n",
        "print(\"AutoML config created.\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ws.get_default_datastore()\n",
        "metrics_output_name = 'metrics_output'\n",
        "best_model_output_name = 'best_model_output'\n",
        "\n",
        "metrics_data = PipelineData(name='metrics_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=metrics_output_name,\n",
        "                           training_output=TrainingOutput(type='Metrics'))\n",
        "model_data = PipelineData(name='model_data',\n",
        "                           datastore=ds,\n",
        "                           pipeline_output_name=best_model_output_name,\n",
        "                           training_output=TrainingOutput(type='Model'))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_automlStep = AutoMLStep(name='AutoML_Classification',\n",
        "                                 automl_config=automl_config,\n",
        "                                 outputs=[metrics_data, model_data],\n",
        "                                 allow_reuse=True)\n",
        "\n",
        "print(\"trainWithAutomlStep created.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "reg_comp_name = \"<<compute_name>>\"\n",
        "reg_compute_target = ComputeTarget(workspace=ws, name=reg_comp_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "source_directory"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-sdk\")\n",
        "\n",
        "rcfg = RunConfiguration(conda_dependencies=conda_dep)\n",
        "\n",
        "register_model_step = PythonScriptStep(script_name='register_model.py',\n",
        "                                       source_directory=source_directory,\n",
        "                                       name=\"Register_Best_Model\",\n",
        "                                       inputs=[model_data],\n",
        "                                               # ds_step_1_train.parse_parquet_files().as_named_input('input_train'), \n",
        "                                               # ds_step_1_test.parse_parquet_files().as_named_input('input_test')],\n",
        "                                       compute_target=reg_compute_target,\n",
        "                                       arguments=[\"--saved-model\", model_data, \n",
        "                                                  '--model-name' , 'titanic_model', \n",
        "                                                  '--featureset-name-train', feature_dataset_name_train, \n",
        "                                                  '--featureset-name-test', feature_dataset_name_test],\n",
        "                                       allow_reuse=True,\n",
        "                                       runconfig=rcfg)\n",
        "\n",
        "# register_model_step.run_after(train_automlStep)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [dbNbStep, train_automlStep, register_model_step]\n",
        "pipeline = Pipeline(workspace=ws, steps=steps)\n",
        "pipeline_run = Experiment(ws, 'DB_FeatureStore_AutoML_Register').submit(pipeline)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1651694252978
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_run.wait_for_completion()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to access the model dataset properties in the production setting for deployment or model consumption\n",
        "\n",
        "Once the pipeline is completed, then you can access the `dataset` information from the registered model by accessing the `datasets` properties of the registered model. In this example, you'll recieve a dictionary that the key is the name provided when the model was registered, `featurized data` in this case.\n",
        "\n",
        "This is helpful if deployment setting of the retrieving dataset characteristics is important. In addition, you can use this method if you like to access the model and dataset information from outside of AML like Databricks or Kubernetes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "model = Model(ws, name='titanic_model')\n",
        "model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_datasets = model.datasets\n",
        "featurized_data = model_datasets['featurized training data'][0]\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featurized_data.tags"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = featurized_data.to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing the Dataset from outside of AML\n",
        "\n",
        "In some use-cases, you might want to access the AML Dataset from outside of AML such as Databricks. In order to do this, you can either access the registered data from the `Databricks Feature Store` as is provided in the first step in `DatabricksStep`, or simply calling the `Dataset.get_by_name` function to retrieve the dataset object and start exploring."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ds_feature_dataset = Dataset.get_by_name(ws, feature_dataset_name)\n",
        "\n",
        "pdf_feature_dataset = ds_feature_dataset.to_pandas_dataframe()\n",
        "pdf_feature_dataset.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}